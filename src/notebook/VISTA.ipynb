{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gp4gdiMfY42m",
        "outputId": "1491996c-1f10-4617-f4c8-41cd8eef39df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Processing audio files and segmenting into 2-second clips...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Audio Files: 100%|██████████| 56/56 [00:29<00:00,  1.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of 2-second clips: 3439\n",
            "Labels range: 0 to 55 (should be 0 to 55)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VACL Pre-training Epoch 0: 100%|██████████| 430/430 [2:01:31<00:00, 16.96s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VACL Pre-training Epoch 0, Loss: 0.0066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VACL Pre-training Epoch 1: 100%|██████████| 430/430 [2:03:36<00:00, 17.25s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VACL Pre-training Epoch 1, Loss: 0.0050\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "VACL Pre-training Epoch 2:  43%|████▎     | 183/430 [52:44<1:08:10, 16.56s/it]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import WavLMModel, Wav2Vec2FeatureExtractor\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import roc_curve\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, audio_files, labels, processor):\n",
        "        self.audio_files = audio_files\n",
        "        self.labels = labels\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        waveform, sr = torchaudio.load(self.audio_files[idx])\n",
        "        waveform = torchaudio.transforms.Resample(sr, 16000)(waveform)\n",
        "        if waveform.dim() == 2:\n",
        "            waveform = waveform.squeeze(0)\n",
        "        if torch.isnan(waveform).any() or torch.isinf(waveform).any():\n",
        "            print(f\"Invalid waveform at index {idx}\")\n",
        "        inputs = self.processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "        return inputs[\"input_values\"].squeeze(0), self.labels[idx]\n",
        "\n",
        "def segment_audio(audio_path, segment_length=2, sample_rate=16000):\n",
        "    waveform, sr = torchaudio.load(audio_path)\n",
        "    if sr != sample_rate:\n",
        "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
        "    if waveform.dim() == 2 and waveform.size(0) > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)\n",
        "    elif waveform.dim() == 1:\n",
        "        waveform = waveform.unsqueeze(0)\n",
        "    samples_per_segment = segment_length * sample_rate\n",
        "    total_samples = waveform.size(1)\n",
        "    segments = []\n",
        "    for start in range(0, total_samples, samples_per_segment):\n",
        "        end = min(start + samples_per_segment, total_samples)\n",
        "        if end - start == samples_per_segment:\n",
        "            segment = waveform[:, start:end]\n",
        "            segments.append(segment)\n",
        "    return segments\n",
        "\n",
        "def prepare_dataset(dataset_dir, output_dir=\"/content/segmented_audio\"):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    audio_files = []\n",
        "    labels = []\n",
        "    filenames = [f for f in os.listdir(dataset_dir) if f.endswith('.wav')]\n",
        "    for filename in tqdm(filenames, desc=\"Processing Audio Files\"):\n",
        "        speaker_id = int(filename[:2]) - 1\n",
        "        if 0 <= speaker_id < 56:\n",
        "            file_path = os.path.join(dataset_dir, filename)\n",
        "            waveform, sr = torchaudio.load(file_path)\n",
        "            duration = waveform.size(1) / sr\n",
        "            if duration < 2:\n",
        "                print(f\"Skipping {filename}: Duration {duration:.2f}s < 2s\")\n",
        "                continue\n",
        "            segments = segment_audio(file_path)\n",
        "            for i, segment in enumerate(segments):\n",
        "                segment_path = os.path.join(output_dir, f\"{filename[:-4]}_seg{i}.wav\")\n",
        "                torchaudio.save(segment_path, segment, 16000)\n",
        "                audio_files.append(segment_path)\n",
        "                labels.append(speaker_id)\n",
        "    return audio_files, labels\n",
        "\n",
        "class VAMA(nn.Module):\n",
        "    def __init__(self, hidden_size=768, num_scales=3):\n",
        "        super(VAMA, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_scales = num_scales\n",
        "        self.context_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, num_scales)\n",
        "        )\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(hidden_size, hidden_size, kernel_size=3, padding=\"same\")\n",
        "            for _ in range(num_scales)\n",
        "        ])\n",
        "        self.attn_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size // 2, 1)\n",
        "        )\n",
        "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
        "        self.beta_param = nn.Parameter(torch.tensor(0.5))\n",
        "        self.register_buffer(\"speaker_mean\", torch.zeros(hidden_size))\n",
        "        self.register_buffer(\"speaker_var\", torch.ones(hidden_size))\n",
        "        self.momentum = 0.1\n",
        "\n",
        "    def estimate_mi(self, features, labels):\n",
        "        clf = LinearSVC(max_iter=1000)\n",
        "        feats_flat = features.mean(dim=1).detach().cpu().numpy()\n",
        "        clf.fit(feats_flat, labels.cpu().numpy())\n",
        "        mi_score = clf.score(feats_flat, labels.cpu().numpy())\n",
        "        return torch.tensor(mi_score, device=features.device)\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        batch, time, hidden = x.shape\n",
        "        context = x.mean(dim=1)\n",
        "        dilation_rates = F.softplus(self.context_mlp(context))\n",
        "        x = x.transpose(1, 2)\n",
        "        scale_feats = []\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            dilation = int(dilation_rates[:, i].mean().item() + 1)\n",
        "            conv.dilation = (dilation,)\n",
        "            scale_feats.append(conv(x).transpose(1, 2))\n",
        "        weights = []\n",
        "        for feat in scale_feats:\n",
        "            attn_score = self.attn_mlp(feat.mean(dim=1)).squeeze(-1)\n",
        "            if labels is not None:\n",
        "                mi_score = self.estimate_mi(feat, labels)\n",
        "                attn_score = attn_score + 0.1 * mi_score\n",
        "            weights.append(attn_score)\n",
        "        weights = torch.stack(weights, dim=1)\n",
        "        weights = F.softmax(weights, dim=1)\n",
        "        weights = weights.unsqueeze(2).unsqueeze(3)\n",
        "        scale_feats = torch.stack(scale_feats, dim=1)\n",
        "        fused = (weights * scale_feats).sum(dim=1)\n",
        "        clip_mean, clip_var = fused.mean(dim=1), fused.var(dim=1)\n",
        "        if self.training and labels is not None:\n",
        "            unique_labels = labels.unique()\n",
        "            for lbl in unique_labels:\n",
        "                mask = (labels == lbl)\n",
        "                if mask.sum() > 0:\n",
        "                    lbl_mean = clip_mean[mask].mean(dim=0)\n",
        "                    lbl_var = clip_var[mask].mean(dim=0)\n",
        "                    self.speaker_mean = (1 - self.momentum) * self.speaker_mean + self.momentum * lbl_mean\n",
        "                    self.speaker_var = (1 - self.momentum) * self.speaker_var + self.momentum * lbl_var\n",
        "        mu = self.alpha * clip_mean + (1 - self.alpha) * self.speaker_mean\n",
        "        sigma = torch.sqrt(self.beta_param * clip_var + (1 - self.beta_param) * self.speaker_var + 1e-6)\n",
        "        normed = self.gamma * (fused - mu.unsqueeze(1)) / sigma.unsqueeze(1) + self.beta\n",
        "        return normed\n",
        "\n",
        "class WavLM_VISTA(nn.Module):\n",
        "    def __init__(self, num_classes=56):\n",
        "        super(WavLM_VISTA, self).__init__()\n",
        "        self.wavlm = WavLMModel.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
        "        self.vama = VAMA(hidden_size=768)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(768, num_classes)\n",
        "        self.adv_heads = nn.ModuleList([\n",
        "            nn.Sequential(nn.Linear(768, 128), nn.ReLU(), nn.Linear(128, 3)),\n",
        "            nn.Sequential(nn.Linear(768, 128), nn.ReLU(), nn.Linear(128, 3))\n",
        "        ])\n",
        "\n",
        "    def forward(self, input_values, attention_mask=None, labels=None, return_adv=False):\n",
        "        outputs = self.wavlm(input_values, attention_mask=attention_mask)\n",
        "        features = outputs.last_hidden_state\n",
        "        embedding = self.pool(features.transpose(1, 2)).squeeze(-1)\n",
        "        if return_adv:\n",
        "            adv_outputs = [head(grad_reverse(embedding, 0.1)) for head in self.adv_heads]\n",
        "            return embedding, adv_outputs\n",
        "        return embedding\n",
        "\n",
        "class GradientReversal(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output.neg() * ctx.alpha, None\n",
        "\n",
        "def grad_reverse(x, alpha=1.0):\n",
        "    return GradientReversal.apply(x, alpha)\n",
        "\n",
        "class ArcFaceLoss(nn.Module):\n",
        "    def __init__(self, s=30.0, m=0.5):\n",
        "        super(ArcFaceLoss, self).__init__()\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "\n",
        "    def forward(self, logits, labels):\n",
        "        theta = torch.acos(torch.clamp(logits, -1.0 + 1e-7, 1.0 - 1e-7))\n",
        "        target_logits = torch.cos(theta + self.m)\n",
        "        one_hot = F.one_hot(labels, num_classes=logits.size(1)).float()\n",
        "        output = self.s * (one_hot * target_logits + (1 - one_hot) * logits)\n",
        "        return F.cross_entropy(output, labels)\n",
        "\n",
        "def pretrain_vacl(model, loader, epochs=10, device=\"cuda\"):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, weight_decay=1e-5)\n",
        "    reconstruction_head = nn.Linear(768, 768).to(device)\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for inputs, _ in tqdm(loader, desc=f\"VACL Pre-training Epoch {epoch}\"):\n",
        "            inputs = inputs.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            emb = model(inputs)\n",
        "            if torch.isnan(emb).any() or torch.isinf(emb).any():\n",
        "                print(\"NaN or Inf detected in embeddings\")\n",
        "            recon = reconstruction_head(emb)\n",
        "            if torch.isnan(recon).any() or torch.isinf(recon).any():\n",
        "                print(\"NaN or Inf detected in reconstruction\")\n",
        "            loss = F.mse_loss(recon, emb.detach())\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"VACL Pre-training Epoch {epoch}, Loss: {total_loss / len(loader):.4f}\")\n",
        "    torch.save(model.state_dict(), \"/content/drive/MyDrive/Speaker Recognition/wavlm_vista_vacl.pth\")\n",
        "\n",
        "def finetune_doai(model, loader, epochs=20, device=\"cuda\"):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "    arcface_loss = ArcFaceLoss(s=30.0, m=0.5)\n",
        "    ce_loss = nn.CrossEntropyLoss()\n",
        "    alpha = 0.01\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for inputs, labels in tqdm(loader, desc=f\"DOAI Fine-tuning Epoch {epoch}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            emb, adv_outputs = model(inputs, labels=labels, return_adv=True)\n",
        "            logits = model.fc(emb)\n",
        "            arc_loss = arcface_loss(logits, labels)\n",
        "            pseudo_emotion = torch.randint(0, 3, (inputs.size(0),), device=device)\n",
        "            pseudo_rate = torch.randint(0, 3, (inputs.size(0),), device=device)\n",
        "            adv_loss = ce_loss(adv_outputs[0], pseudo_emotion) + ce_loss(adv_outputs[1], pseudo_rate)\n",
        "            loss = arc_loss - alpha * adv_loss\n",
        "            if loss.item() < -1.0:\n",
        "                print(f\"Early stopping at epoch {epoch} due to negative loss: {loss.item()}\")\n",
        "                break\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"DOAI Epoch {epoch}, Loss: {total_loss / len(loader):.4f}\")\n",
        "    torch.save(model.state_dict(), \"/content/drive/MyDrive/Speaker Recognition/wavlm_vista_doai.pth\")\n",
        "\n",
        "def evaluate_metrics(model, loader, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    embeddings, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, lbls in tqdm(loader, desc=\"Evaluating Metrics\"):\n",
        "            inputs, lbls = inputs.to(device), lbls\n",
        "            emb = model(inputs, labels=lbls)\n",
        "            embeddings.append(emb.cpu())\n",
        "            labels.append(lbls.cpu())\n",
        "    embeddings = torch.cat(embeddings)\n",
        "    labels = torch.cat(labels)\n",
        "    clf = LinearSVC(max_iter=1000)\n",
        "    clf.fit(embeddings.numpy(), labels.numpy())\n",
        "    mi_score = clf.score(embeddings.numpy(), labels.numpy())\n",
        "    lin_probe_acc = mi_score\n",
        "    model.train()\n",
        "    adv_accs = []\n",
        "    for inputs, _ in tqdm(loader, desc=\"Adversarial Evaluation\"):\n",
        "        inputs = inputs.to(device)\n",
        "        _, adv_outputs = model(inputs, return_adv=True)\n",
        "        for adv_out in adv_outputs:\n",
        "            pred = adv_out.argmax(dim=1)\n",
        "            acc = (pred == torch.randint(0, 3, pred.size(), device=device)).float().mean().item()\n",
        "            adv_accs.append(acc)\n",
        "    adv_failure = np.mean(adv_accs)\n",
        "    intra_dist, inter_dist = [], []\n",
        "    for i in tqdm(range(len(labels)), desc=\"Calculating Distances\"):\n",
        "        same = embeddings[labels == labels[i]]\n",
        "        diff = embeddings[labels != labels[i]]\n",
        "        intra_dist.append(F.cosine_similarity(embeddings[i:i+1], same).mean().item())\n",
        "        inter_dist.append(F.cosine_similarity(embeddings[i:i+1], diff).mean().item())\n",
        "    compactness = np.mean(intra_dist) / np.mean(inter_dist)\n",
        "    scores = F.cosine_similarity(embeddings.unsqueeze(1), embeddings.unsqueeze(0)).flatten()\n",
        "    true_labels = (labels.unsqueeze(1) == labels.unsqueeze(0)).flatten()\n",
        "    fpr, tpr, _ = roc_curve(true_labels.numpy(), scores.numpy())\n",
        "    eer = fpr[np.nanargmin(np.abs(fpr - (1 - tpr)))]\n",
        "    from sklearn.manifold import TSNE\n",
        "    tsne = TSNE(n_components=2).fit_transform(embeddings.numpy())\n",
        "    hist_train, _ = np.histogramdd(tsne[:len(loader.dataset)//2], bins=20)\n",
        "    hist_test, _ = np.histogramdd(tsne[len(loader.dataset)//2:], bins=20)\n",
        "    kl_div = entropy(hist_train.flatten() + 1e-10, hist_test.flatten() + 1e-10)\n",
        "    print(f\"MI Score: {mi_score:.4f}, Linear Probing: {lin_probe_acc:.4f}\")\n",
        "    print(f\"Adversarial Failure: {adv_failure:.4f}, Compactness: {compactness:.4f}\")\n",
        "    print(f\"EER: {eer:.4f}, t-SNE Divergence: {kl_div:.4f}\")\n",
        "    return mi_score, lin_probe_acc, adv_failure, compactness, eer, kl_div\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    dataset_dir = \"/content/drive/MyDrive/Speaker Recognition/Dataset\"\n",
        "    processor = Wav2Vec2FeatureExtractor.from_pretrained(\"microsoft/wavlm-base-plus\")\n",
        "    print(\"Processing audio files and segmenting into 2-second clips...\")\n",
        "    audio_files, labels = prepare_dataset(dataset_dir)\n",
        "    labels = torch.tensor(labels)\n",
        "    dataset = AudioDataset(audio_files, labels, processor)\n",
        "    loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "    print(f\"Total number of 2-second clips: {len(dataset)}\")\n",
        "    print(f\"Labels range: {labels.min()} to {labels.max()} (should be 0 to 55)\")\n",
        "    model = WavLM_VISTA(num_classes=56).to(device)\n",
        "    pretrain_vacl(model, loader, epochs=10, device=device)\n",
        "    finetune_doai(model, loader, epochs=20, device=device)\n",
        "    metrics = evaluate_metrics(model, loader, device=device)\n",
        "    with open(\"/content/drive/MyDrive/Speaker Recognition/dataset_summary.txt\", \"w\") as f:\n",
        "        for file, label in zip(audio_files, labels):\n",
        "            f.write(f\"File: {file}, Label: {label}\\n\")\n",
        "    print(\"Dataset summary saved to /content/drive/MyDrive/Speaker Recognition/dataset_summary.txt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
